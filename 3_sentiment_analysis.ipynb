{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439cd2e1",
   "metadata": {},
   "source": [
    "# ü§ñ 3. Sentiment Analysis dengan IndoBERT\n",
    "\n",
    "Notebook ini akan memprediksi sentimen menggunakan model **IndoBERT** (`w11wo/indobert-sentiment-classification`).\n",
    "\n",
    "**Model:** Pre-trained IndoBERT untuk sentiment analysis bahasa Indonesia  \n",
    "**Output:** 3 kelas sentimen (negatif, netral, positif) + probabilitas\n",
    "\n",
    "**Catatan:** First run akan download model (~400MB). Pastikan koneksi internet stabil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üñ•Ô∏è Device: {device}\")\n",
    "print(f\"üî¢ PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Using CPU (inference will be slower)\")\n",
    "\n",
    "# Buat folder\n",
    "Path('data/processed').mkdir(parents=True, exist_ok=True)\n",
    "Path('models').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a8606",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load data yang sudah dibersihkan dari notebook sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_file = 'data/interim/all_apps_clean.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"‚úÖ Loaded data: {len(df)} reviews\")\n",
    "    print(f\"üìä Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nüìã Preview:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File not found: {data_file}\")\n",
    "    print(\"‚ö†Ô∏è Jalankan notebook 2_preprocessing.ipynb terlebih dahulu!\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5386ab",
   "metadata": {},
   "source": [
    "## Load Model IndoBERT\n",
    "\n",
    "Download dan load model dari Hugging Face. **First run akan download ~400MB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = 'w11wo/indobert-sentiment-classification'\n",
    "CACHE_DIR = 'models'\n",
    "\n",
    "print(f\"üì• Loading model: {MODEL_NAME}\")\n",
    "print(\"‚è≥ Please wait... (first run akan download model)\")\n",
    "print()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "print(\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"‚úÖ Model loaded and moved to {device}\")\n",
    "\n",
    "# Label mapping\n",
    "LABEL_MAP = {\n",
    "    0: 'negatif',\n",
    "    1: 'netral',\n",
    "    2: 'positif'\n",
    "}\n",
    "print(f\"\\nüè∑Ô∏è Label mapping: {LABEL_MAP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cbd505",
   "metadata": {},
   "source": [
    "## Test Prediksi\n",
    "\n",
    "Test model dengan beberapa contoh teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text, max_length=256):\n",
    "    \"\"\"\n",
    "    Prediksi sentimen untuk satu teks.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'label': str, 'probs': dict}\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=True\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get prediction\n",
    "    pred_idx = torch.argmax(probs, dim=-1).item()\n",
    "    pred_label = LABEL_MAP[pred_idx]\n",
    "    \n",
    "    # Get probabilities\n",
    "    probs_dict = {\n",
    "        LABEL_MAP[i]: float(probs[0][i].cpu())\n",
    "        for i in range(len(LABEL_MAP))\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'label': pred_label,\n",
    "        'probs': probs_dict\n",
    "    }\n",
    "\n",
    "# Test dengan beberapa contoh\n",
    "print(\"üß™ TEST PREDIKSI\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_samples = [\n",
    "    \"aplikasi sangat bagus dan membantu\",\n",
    "    \"aplikasi sering error dan lambat\",\n",
    "    \"biasa saja tidak ada yang istimewa\"\n",
    "]\n",
    "\n",
    "for text in test_samples:\n",
    "    result = predict_text(text)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Prediction: {result['label'].upper()}\")\n",
    "    print(f\"Probabilities:\")\n",
    "    for label, prob in result['probs'].items():\n",
    "        print(f\"  {label}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cffd72b",
   "metadata": {},
   "source": [
    "## Batch Prediction\n",
    "\n",
    "Prediksi sentimen untuk seluruh dataset. **Estimasi waktu:**\n",
    "- CPU: 30-45 menit (untuk 5000+ reviews)\n",
    "- GPU: 3-5 menit\n",
    "\n",
    "**Tips:** Jika terlalu lama, test dulu dengan sample kecil (ubah `df.head(100)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87efb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(texts, batch_size=16, max_length=256):\n",
    "    \"\"\"\n",
    "    Prediksi batch dengan progress bar.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts\n",
    "        batch_size: Batch size (reduce jika out of memory)\n",
    "        max_length: Max sequence length\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with predictions\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'pred_label': [],\n",
    "        'p_negatif': [],\n",
    "        'p_netral': [],\n",
    "        'p_positif': []\n",
    "    }\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Predicting\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=True\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Store results\n",
    "        pred_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "        probs_array = probs.cpu().numpy()\n",
    "        \n",
    "        for pred_idx, prob_array in zip(pred_indices, probs_array):\n",
    "            results['pred_label'].append(LABEL_MAP[pred_idx])\n",
    "            results['p_negatif'].append(float(prob_array[0]))\n",
    "            results['p_netral'].append(float(prob_array[1]))\n",
    "            results['p_positif'].append(float(prob_array[2]))\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Konfigurasi\n",
    "BATCH_SIZE = 16  # Reduce jika out of memory (misal 8 atau 4)\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "print(f\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Max length: {MAX_LENGTH}\")\n",
    "print(f\"   Total reviews: {len(df)}\")\n",
    "print()\n",
    "\n",
    "# OPTIONAL: Test dengan sample kecil dulu\n",
    "# Uncomment baris di bawah untuk test dengan 100 data pertama\n",
    "# df_to_predict = df.head(100).copy()\n",
    "# print(\"‚ö†Ô∏è TEST MODE: Using first 100 reviews only\")\n",
    "\n",
    "# Untuk full dataset, gunakan ini:\n",
    "df_to_predict = df.copy()\n",
    "\n",
    "print(\"üöÄ Starting prediction...\")\n",
    "print(f\"‚è±Ô∏è Estimated time: {'3-5 minutes (GPU)' if device == 'cuda' else '30-45 minutes (CPU)'}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd58f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jalankan prediksi\n",
    "texts = df_to_predict['clean_text'].fillna('').tolist()\n",
    "\n",
    "predictions_df = predict_batch(\n",
    "    texts, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Prediction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63897d",
   "metadata": {},
   "source": [
    "## Gabungkan Hasil dengan Data Asli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643219f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan prediksi dengan data asli\n",
    "df_result = pd.concat([\n",
    "    df_to_predict.reset_index(drop=True),\n",
    "    predictions_df\n",
    "], axis=1)\n",
    "\n",
    "print(f\"‚úÖ Combined data shape: {df_result.shape}\")\n",
    "print(f\"\\nüìã Columns: {list(df_result.columns)}\")\n",
    "print(f\"\\nüîç Preview:\")\n",
    "print(df_result[['app', 'content', 'score', 'label', 'pred_label', 'p_positif', 'p_netral', 'p_negatif']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c028525d",
   "metadata": {},
   "source": [
    "## Analisis Hasil Prediksi\n",
    "\n",
    "Bandingkan label otomatis (dari rating) vs prediksi model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribusi prediksi\n",
    "print(\"üìä DISTRIBUSI PREDIKSI\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pred_counts = df_result['pred_label'].value_counts()\n",
    "for label, count in pred_counts.items():\n",
    "    percentage = count / len(df_result) * 100\n",
    "    print(f\"{label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Distribusi per app\n",
    "print(\"\\nüì± DISTRIBUSI PER APLIKASI\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for app in df_result['app'].unique():\n",
    "    app_data = df_result[df_result['app'] == app]\n",
    "    print(f\"\\n{app.upper()}:\")\n",
    "    for label, count in app_data['pred_label'].value_counts().items():\n",
    "        percentage = count / len(app_data) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Perbandingan label vs prediksi\n",
    "print(\"\\nüîÑ AGREEMENT: Label (Rating) vs Prediction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "agreement = (df_result['label'] == df_result['pred_label']).sum()\n",
    "total = len(df_result)\n",
    "agreement_pct = agreement / total * 100\n",
    "\n",
    "print(f\"Agreement: {agreement}/{total} ({agreement_pct:.2f}%)\")\n",
    "print(f\"Disagreement: {total-agreement}/{total} ({100-agreement_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cabe11",
   "metadata": {},
   "source": [
    "## Contoh Prediksi\n",
    "\n",
    "Lihat contoh prediksi per kategori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c22b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã CONTOH PREDIKSI PER KATEGORI\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for pred_label in ['positif', 'netral', 'negatif']:\n",
    "    print(f\"\\nüè∑Ô∏è PREDIKSI: {pred_label.upper()}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    samples = df_result[df_result['pred_label'] == pred_label].head(3)\n",
    "    for idx, row in samples.iterrows():\n",
    "        print(f\"App: {row['app']} | Rating: {row['score']} | Label: {row['label']}\")\n",
    "        print(f\"Text: {row['content'][:100]}...\")\n",
    "        print(f\"Confidence: {row[f'p_{pred_label}']:.2%}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8cf81",
   "metadata": {},
   "source": [
    "## Simpan Hasil\n",
    "\n",
    "Simpan data dengan prediksi ke `data/processed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ab9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan hasil\n",
    "output_file = 'data/processed/all_apps_with_predictions.csv'\n",
    "df_result.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"üíæ Saved results to: {output_file}\")\n",
    "print(f\"üìä Total rows: {len(df_result)}\")\n",
    "print(f\"üìÇ File size: {Path(output_file).stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620a376e",
   "metadata": {},
   "source": [
    "## üéâ Selesai!\n",
    "\n",
    "Prediksi sentimen berhasil dilakukan!\n",
    "\n",
    "**Output:**\n",
    "- File: `data/processed/all_apps_with_predictions.csv`\n",
    "- Total: {len(df_result)} reviews\n",
    "- Kolom prediksi: `pred_label`, `p_negatif`, `p_netral`, `p_positif`\n",
    "\n",
    "**Next steps:**\n",
    "- Jalankan notebook `4_evaluation.ipynb` untuk evaluasi mendalam & visualisasi\n",
    "\n",
    "**Summary:**\n",
    "- ‚úÖ Model: IndoBERT sentiment classification\n",
    "- ‚úÖ Prediksi: {len(df_result)} reviews\n",
    "- ‚úÖ Device: {device}\n",
    "- ‚úÖ Distribusi: {dict(pred_counts)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
